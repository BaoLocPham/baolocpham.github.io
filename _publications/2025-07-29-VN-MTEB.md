---
title: "VN-MTEB: Vietnamese Massive Text Embedding Benchmark"
collection: publications
category: conferences
permalink: /publication/2025-07-29-VN-MTEB
excerpt: 'This paper is about the VN-MTEB'
date: 2025-07-29
venue: 'EACL 2026'
slidesurl: '/files/HCMC_AI_MEET_UP_VN-MTEB.pdf'
paperurl: '/files/papers/425_VN_MTEB_Vietnamese_Massive.pdf'
citation: 'Pham, L., Luu, T., Vo, T., Nguyen, M., & Hoang, V. (2025, July 29). VN-MTEB: Vietnamese Massive Text Embedding Benchmark. arXiv.org. https://arxiv.org/abs/2507.21500'
---

Vietnam ranks among the top countries in terms of both internet traffic and online toxicity. As a result, implementing embedding models for recommendation and content control duties in applications is crucial. However, a lack of large-scale test datasets, both in volume and task diversity, makes it tricky for scientists to effectively evaluate AI models before deploying them in real-world, large-scale projects. To solve this important problem, we introduce a Vietnamese benchmark, VN-MTEB for embedding models, which we created by translating a large number of English samples from the Massive Text Embedding Benchmark using our new automated framework. We leverage the strengths of large language models (LLMs) and cutting-edge embedding models to conduct translation and filtering processes to retain high-quality samples, guaranteeing a natural flow of language and semantic fidelity while preserving named entity recognition (NER) and code snippets. Our comprehensive benchmark consists of 41 datasets from six tasks specifically designed for Vietnamese text embeddings. In our analysis, we find that bigger and more complex models using Rotary Positional Embedding outperform those using Absolute Positional Embedding in embedding tasks. Datasets are available at HuggingFace: https://huggingface.co/collections/GreenNode/vn-mteb